{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fbc463d",
   "metadata": {},
   "source": [
    "### RGMA GPM + feature labels product\n",
    "- some example figures demosntrating the overlapping rainfall from multiple features\n",
    "- associated statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5631f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeat\n",
    "from cartopy.util import add_cyclic_point\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f56f6ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42adffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinates_processors(data):\n",
    "    \"\"\"\n",
    "    converting longitude/latitude into lon/lat\n",
    "    data: xarray.dataset coordinated horizontally in lat/lon\n",
    "    \"\"\"\n",
    "\n",
    "    coord_names = []\n",
    "    for coord_name in data.coords:\n",
    "        coord_names.append(coord_name)\n",
    "\n",
    "    if (set(coord_names) & set(['lon','lat'])): # if coordinates set this way...\n",
    "\n",
    "        data2 = data.rename({'lat': 'latitude'})\n",
    "        data2 = data2.rename({'lon': 'longitude'})\n",
    "\n",
    "    else:\n",
    "        data2 = data\n",
    "\n",
    "    # check if lon from -180\n",
    "    if data2.longitude[0] != 0: # -180 to 180\n",
    "\n",
    "        lon_reset = data2.longitude\n",
    "        lon_reset = lon_reset.where(lon_reset > 0, 360+lon_reset) # converting lon as 0 to 359.75\n",
    "        data2.coords['longitude'] = lon_reset # converting lon as -180 to 180\n",
    "        data2= data2.sortby('longitude')\n",
    "\n",
    "    # check if latitutde is decreasing\n",
    "    if (data2.latitude[1] - data2.latitude[0]) < 0:\n",
    "        data2 = data2.isel(latitude=slice(None, None, -1)) # flipping latitude accoordingly\n",
    "\n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f13a31b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare directories\n",
    "RGMA_DIR = Path('/neelin2020/RGMA_feature_mask/')\n",
    "GPM_DIR = Path('/neelin2020/RGMA_feature_mask/GPM_ncfiles_2017/')\n",
    "DATA_DIR = RGMA_DIR / 'data_product/2017/'\n",
    "AR_DIR = DATA_DIR / 'AR'\n",
    "MCS_DIR = DATA_DIR / 'MCS'\n",
    "LPS_DIR = DATA_DIR / 'LPS'\n",
    "Front_DIR = DATA_DIR / 'Front'\n",
    "OUT_DIR = DATA_DIR / 'MERGED_FP'\n",
    "\n",
    "lat_range = [-60, 60] # MSC only tracked within 60 deg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fba88509",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mon in range(1,2):\n",
    "\n",
    "    # load feature datasets\n",
    "    files = list(AR_DIR.glob('*_{}_*'.format(str(mon).zfill(2))))[0]\n",
    "    data_AR = xr.open_dataset(files)\n",
    "    data_AR = coordinates_processors(data_AR)\n",
    "    data_AR = data_AR.sel(latitude=slice(lat_range[0], lat_range[1]))\n",
    "\n",
    "    files = list(Front_DIR.glob('Front_cold*_{}_*'.format(str(mon).zfill(2))))[0]\n",
    "    data_FT_c = xr.open_dataset(files)\n",
    "    data_FT_c = coordinates_processors(data_FT_c)\n",
    "    data_FT_c = data_FT_c.sel(latitude=slice(lat_range[0], lat_range[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f41eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load feature datasets\n",
    "data_AR = xr.open_dataset(RGMA_DIR / 'AR_ERA5feature_mask_2017_hrly.nc')\n",
    "data_AR = coordinates_processors(data_AR)\n",
    "data_AR = data_AR.sel(latitude=slice(lat_range[0], lat_range[1]))\n",
    "\n",
    "data_FT_c = xr.open_dataset(RGMA_DIR / 'Front_cold_ERA5feature_mask_2017_6hrly.nc')\n",
    "data_FT_c = coordinates_processors(data_FT_c)\n",
    "data_FT_c = data_FT_c.sel(latitude=slice(lat_range[0], lat_range[1]))\n",
    "\n",
    "data_FT_w = xr.open_dataset(RGMA_DIR / 'Front_warm_ERA5feature_mask_2017_6hrly.nc')\n",
    "data_FT_w = coordinates_processors(data_FT_w)\n",
    "data_FT_w = data_FT_w.sel(latitude=slice(lat_range[0], lat_range[1]))\n",
    "\n",
    "data_FT_s = xr.open_dataset(RGMA_DIR / 'Front_stat_ERA5feature_mask_2017_6hrly.nc')\n",
    "data_FT_s = coordinates_processors(data_FT_s)\n",
    "data_FT_s = data_FT_s.sel(latitude=slice(lat_range[0], lat_range[1]))\n",
    "\n",
    "data_LPS = xr.open_dataset(RGMA_DIR / 'LPS_ERA5feature_mask_2017_hrly.nc')\n",
    "data_LPS = coordinates_processors(data_LPS)\n",
    "data_LPS = data_LPS.sel(latitude=slice(lat_range[0], lat_range[1]))\n",
    "\n",
    "data_MCS = xr.open_dataset(RGMA_DIR / 'MCS_ERA5feature_mask_2017_hrly.nc')\n",
    "data_MCS = coordinates_processors(data_MCS)\n",
    "data_MCS = data_MCS.sel(latitude=slice(lat_range[0], lat_range[1]))\n",
    "\n",
    "file_list = sorted(list(GPM_DIR.glob('GPM_IMERGE_V06_201701*')))\n",
    "data_GPM = xr.open_mfdataset(file_list)\n",
    "data_GPM = coordinates_processors(data_GPM)\n",
    "data_GPM_sub = data_GPM.sel(time=slice(datetime(2017,1,1,0),datetime(2017,12,31,23)),\n",
    "                            latitude=slice(lat_range[0], lat_range[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d079c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset variable name as feature_tag (ar_tag, mcs_tag, lps_tag, front_tg)\n",
    "data_AR = data_AR.rename({'ar_binary_tag': 'ar_tag'})\n",
    "data_FT_c = data_FT_c.rename({'fronts': 'front_c_tag'})\n",
    "data_FT_w = data_FT_w.rename({'fronts': 'front_w_tag'})\n",
    "data_FT_s = data_FT_s.rename({'fronts': 'front_s_tag'})\n",
    "data_LPS = data_LPS.rename({'feature_mask': 'lps_tag'})\n",
    "data_MCS = data_MCS.rename({'feature_mask': 'mcs_tag'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e941392b",
   "metadata": {},
   "source": [
    "### Extra modification for MCS and LPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358a927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_era5 = data_AR.longitude\n",
    "lat_era5 = data_AR.latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75731b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCS is defined at xx:30 based on MERGE-IR, we assume small changes in the feature within \n",
    "# the 30-min time frame for simplicity\n",
    "\n",
    "MCS_array = np.zeros(data_AR.ar_tag.shape)\n",
    "MCS_array[:len(data_MCS.mcs_tag.time),:,:] = data_MCS.mcs_tag.values # fill the new array by the original tag\n",
    "\n",
    "data_MCS_re = xr.Dataset(data_vars=dict(mcs_tag=(['time','latitude','longitude'], MCS_array)),\n",
    "                         coords=dict(time=data_AR.time.values,\n",
    "                                     latitude=(['latitude'], data_MCS.latitude.values),\n",
    "                                     longitude=(['longitude'], data_MCS.longitude.values)))\n",
    "del MCS_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f6774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LPS feature is provided with the feature center, now we simply use a 5-deg box mask to \n",
    "# # present the feature associated boundaries\n",
    "\n",
    "# LPS_array = np.zeros(data_LPS.lps_tag.shape)\n",
    "# range_sel = int(5/0.25) # 5-deg box\n",
    "\n",
    "# for t in range(len(data_LPS.time)):\n",
    "    \n",
    "#     tmp = data_LPS.isel(time=t).lps_tag.values\n",
    "#     (idy_list, idx_list) = np.where(tmp == 1) # find index of LPS\n",
    "    \n",
    "#     for (idy, idx) in zip(idy_list, idx_list):\n",
    "        \n",
    "#         LPS_array[t,(idy-range_sel):(idy+range_sel+1),(idx-range_sel):(idx+range_sel+1)] = 1\n",
    "        \n",
    "# data_LPS_re = xr.Dataset(data_vars=dict(lps_tag=(['time','latitude','longitude'], LPS_array)),\n",
    "#                          coords=dict(time=data_LPS.time.values,\n",
    "#                                      latitude=(['latitude'], data_LPS.latitude.values),\n",
    "#                                      longitude=(['longitude'], data_LPS.longitude.values)))\n",
    "# del LPS_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe53e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate GPM into ERA5 grid using linear fitting\n",
    "data_GPM_intp = data_GPM_sub.interp(longitude=lon_era5, latitude=lat_era5, method='linear').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_GPM_intp = data_GPM_intp.transpose('time','latitude','longitude') # transpose data coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a4e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# final step: merge into a feature-precip (FP) xarray dataset\n",
    "# note: resample 6H due to front data\n",
    "data_FP_merged = xr.merge([data_GPM_intp.resample(time='6H').nearest(),\n",
    "                           data_AR.resample(time='6H').nearest(),\n",
    "                           data_FT_c.resample(time='6H').nearest(),\n",
    "                           data_FT_w.resample(time='6H').nearest(),\n",
    "                           data_FT_s.resample(time='6H').nearest(),\n",
    "                           data_LPS.resample(time='6H').nearest(),\n",
    "                           data_MCS_re.resample(time='6H').nearest()])\n",
    "data_FP_merged['precipitationCal'] = data_FP_merged['precipitationCal'].where(data_FP_merged['precipitationCal'] > 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f602456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tags as int8 for data compression\n",
    "data_FP_merged = data_FP_merged.fillna(0)\n",
    "data_FP_merged['ar_tag'].values = data_FP_merged.ar_tag.values.astype('byte')\n",
    "data_FP_merged['front_c_tag'].values = data_FP_merged.front_c_tag.values.astype('byte')\n",
    "data_FP_merged['front_w_tag'].values = data_FP_merged.front_w_tag.values.astype('byte')\n",
    "data_FP_merged['front_s_tag'].values = data_FP_merged.front_s_tag.values.astype('byte')\n",
    "data_FP_merged['lps_tag'].values = data_FP_merged.lps_tag.values.astype('byte')\n",
    "data_FP_merged['mcs_tag'].values = data_FP_merged.mcs_tag.values.astype('byte')\n",
    "# reassign time\n",
    "data_FP_merged.coords['time'] = data_AR.resample(time='6H').nearest().time.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb35d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save netcdf file by month\n",
    "date_range = pd.date_range(data_FP_merged.time[0].values, data_FP_merged.time[-1], freq='6H')\n",
    "\n",
    "for mon in range(1,13): # slice the full data into monthly data\n",
    "    idx = np.where(date_range.month == mon) # extract specific month\n",
    "    data_FP_month = data_FP_merged.isel(time=idx)\n",
    "    data_FP_merged.to_netcdf(OUT_DIR / 'GPM_feature_merged_{}_{}.nc'.format(date_range[0].year, str(mon).zfill(2)))\n",
    "    print('GPM_feature_merged_{}_{}.nc saved...'.format(date_range[0].year, str(mon).zfill(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d821f8",
   "metadata": {},
   "source": [
    "### Stats from here: some test for feature-associated extractioon \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a2ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like MCS + unexplained over 1, but its not\n",
    "(Unexp_contr+MCS_contr).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d4202f",
   "metadata": {},
   "source": [
    "### some statistics from the precipitation contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594cf144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
